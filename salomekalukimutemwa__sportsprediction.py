# -*- coding: utf-8 -*-
"""SALOMEKALUKIMUTEMWA__SportsPrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18cfjUthlITzkCuNOg7f3r15zfCtkiatk
"""

#Importing necessary library

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

#ignore warnings
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

"""# Data Preprocessing"""

# Read the dataset
fifa_data = pd.read_csv('/content/drive/MyDrive/male_players (legacy).csv')

fifa_data.head()

#sanity check
#checking the shape (number of rows and columns)
fifa_data.shape

#selecting columns that are necessary for our training
for i in fifa_data.columns:
    print(i)

"""Removed columns with URLs and other personal details about players leagues and clubs because they don't affect the player ratings. Keeping them would make the model less accurate.

"""

firstAttributes = fifa_data[['overall', 'potential', 'age', 'height_cm', 'weight_kg', 'preferred_foot','weak_foot','skill_moves','international_reputation','work_rate','body_type','player_tags','player_traits' ]]
secondAttributes= fifa_data.loc[:, 'pace':'goalkeeping_speed']
fifa_data = pd.concat([firstAttributes,secondAttributes], axis=1)

#finding missing values
fifa_data.isnull().sum()/fifa_data.shape[0]*100

"""Removing data with the highest percentage (more than 30%) of missing values

"""

threshold = 0.30*len(fifa_data)
fifa_data = fifa_data.dropna(thresh=threshold,axis= 1)

#confirming the removed data after running the threshold
fifa_data.isnull().sum()/fifa_data.shape[0]*100

fifa_data.info()

#checking for duplicates
fifa_data.duplicated().sum()

#Eploratory Data Analysis (EDA)

#descriptve analysis of the numerical columns
fifa_data.describe().T

fifa_data.head()

fifa_data.tail()

#descriptive statistics of categorical columns
fifa_data.describe(include= "object")

#plotting histograms to see the distribution of the data
for i in fifa_data.select_dtypes(include="number").columns:
    sns.histplot(data= fifa_data, x=i)
    plt.show()

# having boxplots to see outliers
for i in fifa_data.select_dtypes(include="number").columns:
    sns.boxplot(data= fifa_data, x=i)
    plt.show()

"""Scatterplot to show how strongly different variables are related to each other.

"""

#removing categorical values
foot = fifa_data.pop('preferred_foot')
workRate = fifa_data.pop('work_rate')
bodyType = fifa_data.pop('body_type')
traits = fifa_data.pop('player_traits')

corr_matrix = fifa_data.corr()
corr_matrix

#creating a heat map to understand the data more
plt.figure(figsize=(10,10))
sns.heatmap(corr_matrix)

"""#Imputation/ treating missing values
Choosing method for imputation depending on whether the data is categorical or numerical.
"""

#before choosing method for imputation, we need to separate dependent variable from the independent variable because we do not do imputation on dependent variables
Y = fifa_data['overall']
fifa_data.drop('overall',axis=1, inplace=True)

#separating numerical and categorical variables

numeric_data= fifa_data.select_dtypes(include = np.number)
non_numeric = fifa_data.select_dtypes(include = ['object'])

categorical_data = fifa_data[non_numeric]

#multivariate imputation (recommended )

from sklearn.experimental import enable_iterative_imputer
from sklearn.impute import IterativeImputer

imp = IterativeImputer (max_iter=10, random_state=0)#assuming that in my data there is an order
numeric_data = pd.DataFrame(np.round(imp.fit_transform(numeric_data)),columns= numeric_data.copy().columns) #numeric data is holding all numeric values without missing values

fifa_data.columns

fifa_data.info()

fifa_data['mentality_composure'].isnull().sum()

numeric_data['mentality_composure'].isnull().sum() #to confirm if the missing values in the numeric data has been dealt with

fifa_data.info()

#using KNN imputation of numeric variables to remove the NaN variables completely
from sklearn.impute import KNNImputer

knn = KNNImputer (n_neighbors=8)#creates an instance of KNNImputer
imputed_numeric_data = knn.fit_transform(numeric_data)

# Convert the resulting array back to a DataFrame
imputed_numeric_data_df = pd.DataFrame(imputed_numeric_data, columns=numeric_data.columns)
imputed_numeric_data_df

# dealing with categorical values to remove NaN or missing values using simpleImputer.
from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='most_frequent')

imputed_non_numeric = imputer.fit_transform(categorical_data)

# Convert the resulting array back to a DataFrame
imputed_non_numeric_df = pd.DataFrame(imputed_non_numeric, columns=non_numeric.columns)
imputed_non_numeric_df



#encoding of data (imputed_non_numeric)
#start by identifying categories in the various categorical columns

foot.value_counts()

workRate.value_counts()

bodyType.value_counts()

"""Use get_dummies to encode"""

foot = pd.get_dummies(foot, prefix='foot').astype(int)

workRate = pd.get_dummies(workRate, prefix='workRate').astype(int)

bodyType = pd.get_dummies(bodyType, prefix='bodyType').astype(int)

#concatenating the imputed numeric data with the encoded categorical data

cleaned_data = pd.concat([imputed_numeric_data_df,foot],axis=1).reset_index(drop=True)

cleaned_data = pd.concat([imputed_numeric_data_df,workRate],axis=1).reset_index(drop=True)

cleaned_data = pd.concat([imputed_numeric_data_df,bodyType],axis=1).reset_index(drop=True)

cleaned_data

X = pd.concat([fifa_data,numeric_data],axis=1).reset_index(drop=True)

X.head()

"""# 2. Feature Engineering

Determining the feature importance by creating feature subsets which have better performance with overall rating.

Analyzing feature importance based on correlation matrix
We are using the target variable 'overall'
"""

correlated_matrix = cleaned_data.corrwith(Y).abs()
sorted_corr = correlated_matrix.sort_values(ascending=False)
sorted_corr

"""Selecting the top 35 features based on their correlation with the overall rating to create a subset of the data focused on the most relevant factors."""

num = 35
top_features = sorted_corr.index[:num]
feature_subset = cleaned_data[top_features]
feature_subset

# using random forest classifier to sort the most important features and reduce overfitting
from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier()
#we should train the model
model.fit(cleaned_data, Y)

#accessing the feature importances
feature_importance = model.feature_importances_

#visualizing the feature importance
plt.figure (figsize=(11,7))
plt.bar(range(len(feature_importance)), feature_importance, tick_label=cleaned_data.columns, color='orange')
plt.xlabel('Features')
plt.ylabel('Importance')
plt.title('RandomForest Classifier Feature Importance')
plt.xticks(rotation='vertical')
plt.show()

"""Sorting and ranking features based on the results in the barchart above.
Sorting will be done in descending order then provide a readable output message for all cleaned data columns
"""

features = cleaned_data.columns

# Sort feature importance in descending order
sorted_indices = np.argsort(feature_importance)[::-1]

print("Feature ranking based on Random Forest Classifier:")
for rank, idx in enumerate(sorted_indices, 1):
    print(f"{rank}. {features[idx]}: {feature_importance[idx]:.4f}")

#Selecting the best 10 features using PCA from the model created based on the correlation matrix.
from sklearn.decomposition import PCA
from sklearn.feature_selection import SelectKBest, f_regression
#using features and my dependent variable, Y

# using SelectKBest with f_regression to select the best features
kbest = SelectKBest(score_func=f_regression, k=10)
best_features = kbest.fit_transform(feature_subset, Y)

#getting column names from the feature subset
best_feature_indices = kbest.get_support(indices=True)
best_feature_names = feature_subset.columns[best_feature_indices]

#initialize PCA model with the desired number of components (10)
pca = PCA(n_components=10)
#fitting and transforming the new model to the best featurs selected by selectKBest
selected_features_pca = pca.fit_transform(best_features)

# Get the explained variance of each principal component
explained_variance = pca.explained_variance_ratio_

# Sort the feature names based on the explained variance (highest variance first)
sorted_feature_names = [name for _, name in sorted(zip(explained_variance, best_feature_names), reverse=True)]

# Select the top N feature names
top_n = 10
selected_feature_names = sorted_feature_names[:top_n]

selected_features_pca = feature_subset[selected_feature_names]
selected_features_pca

#confirming whether the explained variable is 100%. If it is 100% it means that all the important information from the original data has been captured and there is no leftover variability in the data that has not been accounted for.
explained_variance = pca.explained_variance_ratio_
print(f"Explained Variance: {explained_variance.sum() * 100:.2f}%")

"""Scalling the independent variables

Scalling our data to compress the variability of variables for the model to analyze quickly.
"""

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_features = scaler.fit_transform(selected_features_pca)

scaled_dataframe = pd.DataFrame(scaled_features, columns =selected_features_pca.columns)
X = scaled_dataframe
X

"""# 3. Training Models

creating and training a suitable machine learning model with cross-validation that can predict a player's rating

Train atleast 3 models. It can be either RandomForest, XGBoost, Gradient Boost (cross-validation) that can predict a players rating.
"""

from sklearn.model_selection import cross_val_score, train_test_split, RandomizedSearchCV
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, r2_score
from sklearn.tree import DecisionTreeRegressor

Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size = 0.2, random_state = 42)

lin = LinearRegression()

lin.fit(Xtrain, Ytrain)
y_pred = lin.predict(Xtest)

print(f"""mean absolute error: {mean_absolute_error(y_pred,Ytest)},
     mean squared error: {mean_squared_error(y_pred,Ytest)},
     root mean squared error: {np.sqrt(mean_squared_error(y_pred,Ytest))},
     R2 Score: {r2_score(y_pred,Ytest)}
     """)

decTree = DecisionTreeRegressor()

decTree.fit(Xtrain, Ytrain)

dec_pred = decTree.predict(Xtest)

print(f"""mean absolute error: {mean_absolute_error(dec_pred,Ytest)},
     mean squared error: {mean_squared_error(dec_pred,Ytest)},
     root mean squared error: {np.sqrt(mean_squared_error(dec_pred,Ytest))},
     R2 Score: {r2_score(dec_pred,Ytest)}
     """)

# Initializing the regressor
gb_regressor = GradientBoostingRegressor()

parameters =  {
        'n_estimators': [100, 200, 300],
        'max_depth': [3, 4, 5],
        'learning_rate': [0.01, 0.1, 0.2]
    }

r_search = RandomizedSearchCV(gb_regressor, parameters, cv = 3, scoring = 'neg_mean_squared_error')

r_search.fit(Xtrain,Ytrain)

"""# 4. Evaluation

Used MAE or RMSE and then fine tuned model, train and tested it again.
"""

r_search_pred = r_search.predict(Xtest)

print(f"""mean absolute error: {mean_absolute_error(r_search_pred,Ytest)},
     mean squared error: {mean_squared_error(r_search_pred,Ytest)},
     root mean squared error: {np.sqrt(mean_squared_error(r_search_pred,Ytest))},
     R2 Score: {r2_score(r_search_pred,Ytest)}
     """)

"""# 5. Test with new Data set"""

#reading testing dataset
test_data = pd.read_csv('/content/drive/MyDrive/players_22.csv')
copy = test_data.copy()

#dropping the same columns as the training dataset for easy comparison including the 'overall'
test_data = test_data[['movement_reactions','overall','mentality_composure','potential','passing','attacking_short_passing','mentality_vision','international_reputation','skill_long_passing','power_shot_power','physic']]
test_data

"""Removing the useless variables with 30% missing values"""

threshold = 0.30 *len(test_data)

test_data = test_data.dropna(thresh=threshold, axis =1)

#splitting numerical and categorical data

Ytest = test_data['overall']
test_data.drop('overall', axis=1,inplace=True)

numeric_col = test_data.select_dtypes(include=['int64', 'float64']).columns
non_numeric_col = test_data.select_dtypes(include=['object']).columns

nonnumeric_var_test = test_data[non_numeric_col]
numeric_var_test = test_data[numeric_col]

#encoding numerical data

"""Performing KNN Imputation on numerical variables. We'll start by checking the numerical variables in our dataset."""

knn_imputer = KNNImputer(n_neighbors=8)
numeric_var_imputed_2 = knn_imputer.fit_transform(numeric_var_test)
numeric_columns = list(numeric_var_test.columns)
numeric_var_imputed_2 = pd.DataFrame(numeric_var_imputed_2, columns=numeric_col)

"""Concatenating the cleaned numerical data and the rest of the categorical dataset.
We'll rename the numerical datasets as Xtest for testing.
"""

Xtest = numeric_var_imputed_2
Xtest

# scalling independent variables

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()

scaled_features=scaler.fit_transform(Xtest)

"""making a dataFrame with the used and extracted columns"""

order_cols = ['movement_reactions','mentality_composure','potential','passing','attacking_short_passing','mentality_vision','international_reputation','skill_long_passing','power_shot_power','physic']

newX = pd.DataFrame(scaled_features, columns=order_cols)

newX

#evaluating model using new dataset

from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# using randomized search to train regression model
predictions = r_search.predict(newX)

# Calculate multiple metrics
mae, mse, rmse, r2 = mean_absolute_error(Ytest, predictions), mean_squared_error(Ytest, predictions), np.sqrt(mean_squared_error(Ytest, predictions)), r2_score(Ytest, predictions)

# Print the metrics
print(f"MAE: {mae}")
print(f"MSE: {mse}")
print(f"RMSE: {rmse}")
print(f"R2 Score: {r2}")

"""# 6. Deployment"""

import joblib

#save the model
joblib.dump(r_search, 'trained_model.pkl')

['trained_model.pkl']

import sklearn

print(f"NumPy version: {np.__version__}")
print(f"joblib version: {joblib.__version__}")
print(f"pandas version: {sklearn.__version__}")